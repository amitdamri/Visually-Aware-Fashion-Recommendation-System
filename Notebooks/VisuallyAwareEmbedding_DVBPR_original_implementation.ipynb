{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisuallyAwareEmbedding_Remake_AmazonFashion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QgeHRhrEGaN4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXGNlgsARvaV"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyW-QMsbRxcU"
      },
      "source": [
        "# global libraries\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from IPython.display import display\n",
        "# Libraries\n",
        "from io import StringIO, BytesIO\n",
        "import requests\n",
        "from pytz import timezone\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm21EbS-hasl"
      },
      "source": [
        "# CNN Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmKgkm9mGYcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3eb80068-a706-4fee-d6bc-b056127abf13"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3pVLl84Ebtc"
      },
      "source": [
        "# Tensorflow libraries\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D, MaxPooling2D, BatchNormalization, Conv2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Siamese Libraries\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Lambda, Concatenate, Dot, Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVO8Ok577Kp"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGkUHV4Kas9l",
        "outputId": "4f5ebb4d-f8f7-4c93-c3e8-018d962ba057"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnpRHkw6bA16",
        "outputId": "70bf79ad-f4ba-4327-e567-56a4a97d2d66"
      },
      "source": [
        "# change directory to dataset path\n",
        "%cd gdrive/My Drive/visually aware/visually-aware-recommender-system/notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/visually aware/visually-aware-recommender-system/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1ISVktcRaqV"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXI1fuhIRbts"
      },
      "source": [
        "# Load data\n",
        "dataset = np.load(\"AmazonFashion6ImgPartitioned.npy\", \n",
        "                  encoding=\"bytes\", allow_pickle=True)\n",
        "[user_train, user_validation, user_test, items, user_num, item_num] = dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawsvBHKSL8l"
      },
      "source": [
        "# Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uux6JXOKSOB4"
      },
      "source": [
        "# Network params\n",
        "# image size\n",
        "image_width = 224\n",
        "image_height = 224\n",
        "\n",
        "# latent dimensionality K\n",
        "latent_dimensionality = 50\n",
        "\n",
        "# weight decay - conv layer\n",
        "lambda_cnn = 0.001\n",
        "# weight decay - fc layer\n",
        "lambda_fc = 0.001\n",
        "# regularizer for theta_u\n",
        "lambda_u = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHULKb6wAGt"
      },
      "source": [
        "# Training params\n",
        "# epoch params\n",
        "learning_rate = 1e-4\n",
        "validation_sample_count = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1R-QW8JSYxd"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKklfOblSdQl"
      },
      "source": [
        "def CNN(width, height, depth, latent_dim, w_init=\"RandomNormal\", cnn_w_regularizer=None, fc_w_regularizer=None, b_init=\"RandomNormal\"):\n",
        "    \"\"\"\n",
        "    Build the CNN.\n",
        "\n",
        "        :param width (int): Image width in pixels.\n",
        "        :param height (int): The image height in pixels.\n",
        "        :param depth (int): The number of channels for the image.\n",
        "        :param latent_dim (int): Dimesion of the latent space - embedding of the image.\n",
        "        :param w_init=\"he_normal\" (str): The kernel initializer.\n",
        "        :param cnn_w_regularizer=None (str): Regularization method.\n",
        "        :param fc_w_regularizer=None (str): Regularization method.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the model along with the input shape to be\n",
        "    # \"channels last\" and the channels dimension itself\n",
        "    model = Sequential(name='cnn')\n",
        "    input_shape = (height, width, depth)\n",
        "    chan_dim = -1\n",
        "\n",
        "    # if we are using \"channels first\", update the input shape\n",
        "    # and channels dimension\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        input_shape = (depth, height, width)\n",
        "        chan_dim = 1\n",
        "\n",
        "    # conv1\n",
        "    #\n",
        "    # Our first CONV layer will learn a total of 64 filters, each\n",
        "    # of which are 11x11 -- we'll then apply 4x4 strides to reduce\n",
        "    # the spatial dimensions of the volume\n",
        "    # Moreover, a max-pooling layer is added\n",
        "    model.add(Conv2D(64, (11, 11),\n",
        "                      strides=(4, 4),\n",
        "                      padding=\"same\",\n",
        "                      kernel_initializer=w_init,\n",
        "                      kernel_regularizer=cnn_w_regularizer,\n",
        "                      bias_initializer=b_init,\n",
        "                      input_shape=input_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
        "                            padding=\"same\"))\n",
        "\n",
        "    # conv2\n",
        "    #\n",
        "    # Here we stack one more CONV layer on top,\n",
        "    # each layer will learn a total of 256 (5x5) filters\n",
        "    # A max-pooling layer is added\n",
        "    model.add(Conv2D(256, (5, 5),\n",
        "                      padding=\"same\",\n",
        "                      strides=(1, 1),\n",
        "                      kernel_initializer=w_init,\n",
        "                      kernel_regularizer=cnn_w_regularizer,\n",
        "                      bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
        "                            padding=\"same\"))\n",
        "\n",
        "    # conv3\n",
        "    #\n",
        "    # Stack one more CONV layer, keeping 256 total learned filters\n",
        "    # but decreasing the the size of each filter to 3x3\n",
        "    model.add(Conv2D(256, (3, 3),\n",
        "                      padding=\"same\",\n",
        "                      strides=(1, 1),\n",
        "                      kernel_initializer=w_init,\n",
        "                      kernel_regularizer=cnn_w_regularizer,\n",
        "                      bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # Two more CONV layers, same filter size and number\n",
        "    #\n",
        "    # conv4\n",
        "    model.add(Conv2D(256, (3, 3),\n",
        "                      padding=\"same\",\n",
        "                      strides=(1, 1),\n",
        "                      kernel_initializer=w_init,\n",
        "                      kernel_regularizer=cnn_w_regularizer,\n",
        "                      bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # conv5\n",
        "    model.add(Conv2D(256, (3, 3),\n",
        "    \n",
        "                      strides=(1, 1),\n",
        "                      kernel_initializer=w_init,\n",
        "                      kernel_regularizer=cnn_w_regularizer,\n",
        "                      bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
        "                            padding=\"same\"))\n",
        "\n",
        "    # Two fully-connected layers on top of each other\n",
        "    #\n",
        "    # full1\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096,\n",
        "                    kernel_initializer=w_init,\n",
        "                    kernel_regularizer=fc_w_regularizer,\n",
        "                    bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # full2\n",
        "    model.add(Dense(4096,\n",
        "                    kernel_initializer=w_init,\n",
        "                    kernel_regularizer=fc_w_regularizer,\n",
        "                    bias_initializer=b_init))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # full3\n",
        "    model.add(Dense(latent_dim,\n",
        "                    kernel_initializer=w_init,\n",
        "                    kernel_regularizer=fc_w_regularizer,\n",
        "                    bias_initializer=b_init))\n",
        "\n",
        "\n",
        "    # Return the constructed network architecture\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6T60gA3A1Z"
      },
      "source": [
        "# Subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2IdUgf_3Dli"
      },
      "source": [
        "# get user information\n",
        "user_num_original = user_num\n",
        "user_train_original = user_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIlGsAq3Wyw2"
      },
      "source": [
        "# for each batch, force the number of users to be the same\n",
        "batch_size = 128\n",
        "batch_count = int(np.ceil(user_num_original / batch_size))\n",
        "\n",
        "# one complete model will be linked to each user_subset\n",
        "user_subsets = dict(zip(range(batch_count), np.array_split(range(user_num_original), batch_count)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOrfAUZzYx6"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey4GeNg3gq6A"
      },
      "source": [
        "# translate image from byte code to an array and normalize its values from 0-255 to -1 - 1\n",
        "def image_translate(image_bytes,\n",
        "                    image_width=224,\n",
        "                    image_height=224):\n",
        "    img = (np.uint8(np.asarray(Image.open(BytesIO(image_bytes)).convert(\"RGB\").resize((image_height, image_width)))) - 127.5)  / 127.5\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iV5XykGzboh"
      },
      "source": [
        "# get train batches which was sampled uniformly for each user\n",
        "def uniform_train_sample_batch(user_train_ratings, item_images, image_width=224, \n",
        "                               image_height=224, sample=True, batch_size=None, user_idx=None):\n",
        "\n",
        "\n",
        "    if batch_size is not None:\n",
        "        users = range(batch_size)\n",
        "    else:\n",
        "        users = user_idx\n",
        "\n",
        "    triplet_train_batch = {}\n",
        "    \n",
        "    # iterate over each user in subset\n",
        "    for b in users:\n",
        "\n",
        "        # training set\n",
        "        if sample:\n",
        "            u = random.randrange(len(user_train_ratings))\n",
        "        else:\n",
        "            u = b\n",
        "\n",
        "        i = user_train_ratings[u][random.randrange(len(user_train_ratings[u]))][b'productid']\n",
        "        j = random.randrange(len(item_images))\n",
        "        \n",
        "        # while the sampled items observed by the user - sample other item until found non-observed item\n",
        "        while j in [item[b'productid'] for item in user_train_ratings[u]]:\n",
        "            j = random.randrange(len(item_images))\n",
        "\n",
        "        image_i = image_translate(item_images[i][b'imgs'], \n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "        image_j = image_translate(item_images[j][b'imgs'],\n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "        triplet_train_batch[u] = [image_i,\n",
        "                                  image_j]\n",
        "        \n",
        "    return triplet_train_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iNUZdz7YI1Q"
      },
      "source": [
        "# get validation batches which was sampled uniformly for each user\n",
        "def uniform_validation_sample_batch(user_train_ratings,\n",
        "                                    user_validation_ratings,\n",
        "                                    item_images,\n",
        "                                    image_width=224,\n",
        "                                    image_height=224,\n",
        "                                    validation_sample_count=1000,\n",
        "                                    sample=True,\n",
        "                                    batch_size=None,\n",
        "                                    user_idx=None):\n",
        "    \"\"\"\n",
        "    validation_sample_count (int): Number of not-observed items to sample to get the validation set for each user.\n",
        "    \"\"\"\n",
        "\n",
        "    if batch_size is not None:\n",
        "        users = range(batch_size)\n",
        "    else:\n",
        "        users = user_idx\n",
        "\n",
        "    triplet_validation_batch = {}\n",
        "    \n",
        "    # iterate over each user in subset\n",
        "    for b in users:\n",
        "\n",
        "        if sample:\n",
        "            u = random.randrange(len(user_train_ratings))\n",
        "        else:\n",
        "            u = b\n",
        "\n",
        "        # validation set\n",
        "        i = user_validation_ratings[u][0][b'productid']\n",
        "        # get its image\n",
        "        image_i = image_translate(item_images[i][b'imgs'],\n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "\n",
        "        # create a set of all user observed items\n",
        "        reviewed_items = set()\n",
        "        for item in user_train_ratings[u]:\n",
        "            reviewed_items.add(item[b'productid'])\n",
        "        reviewed_items.add(user_validation_ratings[u][0][b'productid'])\n",
        "\n",
        "        # search for a non-observed item\n",
        "        triplet_validation_batch[u] = []\n",
        "        got = False\n",
        "        for j in random.sample(range(len(item_images)), validation_sample_count):\n",
        "            if j not in reviewed_items:\n",
        "                image_j = image_translate(item_images[j][b'imgs'],\n",
        "                                          image_width, \n",
        "                                          image_height)\n",
        "                triplet_validation_batch[u] = [image_i, image_j]\n",
        "                got = True\n",
        "            \n",
        "            if got:\n",
        "              break\n",
        "\n",
        "    return triplet_validation_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1a0Sd0VxutI"
      },
      "source": [
        "# Define the loss function as ln(sigmoid) according to the BPR method\n",
        "# BPR wants to maximize the loss function while Keras engine minimizes it so we added a subtraction \n",
        "def bpr_loss(label_matrix, prediction_matrix):\n",
        "    return 1 - tf.reduce_sum(tf.math.log(tf.sigmoid(prediction_matrix)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_3LyZB0zd5q"
      },
      "source": [
        "# Count the ratio of prediction value > 0\n",
        "# i.e., predicting positive item score > negative item score for a user\n",
        "def auc(label_tensor, prediction_tensor):\n",
        "    return K.mean(K.switch(prediction_tensor > K.zeros_like(prediction_tensor),\n",
        "                           K.ones_like(prediction_tensor),    # 1\n",
        "                           K.zeros_like(prediction_tensor)))  # 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSZneHJBtEDE"
      },
      "source": [
        "# Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixAAscXPtFKN"
      },
      "source": [
        "def ConvSiameseNet(users_dim, width, height, depth,latent_dim, w_init=\"RandomNormal\", cnn_w_regularizer=None, fc_w_regularizer=None, u_w_regularizer=None, b_init=\"RandomNormal\"):\n",
        "\n",
        "        # user ID input  \n",
        "        user_input = Input((1,), name='User')\n",
        "\n",
        "        image_shape = (width, height, depth)\n",
        "        \n",
        "        # observed image\n",
        "        left_input = Input(image_shape,\n",
        "                           name=\"observed_image\")\n",
        "        # non-observed image\n",
        "        right_input = Input(image_shape,\n",
        "                            name=\"non_observed_image\")\n",
        "\n",
        "        # Build convnet to use in each siamese 'leg'\n",
        "        conv_net = CNN(width, height, depth, latent_dim, w_init, fc_w_regularizer, b_init)\n",
        "\n",
        "        # preference layer\n",
        "        preference_relationship = Dot(axes=1,\n",
        "                                      name=\"score_rank\")\n",
        "\n",
        "        # Apply the pipeline to the inputs\n",
        "        # call the convnet Sequential model on each of the input tensors\n",
        "        # so params will be shared\n",
        "        encoded_l = conv_net(left_input)\n",
        "        encoded_r = conv_net(right_input)\n",
        "\n",
        "        # merge the two encoded inputs through the L1 distance\n",
        "        L1_dist = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
        "        \n",
        "        # retrieve the single user preference\n",
        "        user_vec = Flatten(name='FlattenUsers')(Embedding(user_num+1, latent_dimensionality, name = 'User-Embedding', embeddings_initializer= tf.keras.initializers.RandomUniform(0,0.01),\n",
        "                                                          input_length=1, embeddings_regularizer=u_w_regularizer)(user_input))\n",
        "        \n",
        "        # get the preference score\n",
        "        prediction = preference_relationship([user_vec, L1_dist])\n",
        "        \n",
        "        # Create the model\n",
        "        model = Model(inputs=[user_input, left_input, right_input],\n",
        "                      outputs=prediction)\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        \n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qmAPrZ1tO0T"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDlrHUUitOdD",
        "outputId": "3a157a89-73a4-4193-f445-62da9821ef97"
      },
      "source": [
        "# build the model with the regularization hyper-parameters and initializers\n",
        "conv_siamese_net = ConvSiameseNet(users_dim=len(user_subsets[0]),\n",
        "                                        width=image_width,\n",
        "                                        height=image_height,\n",
        "                                        depth=3,\n",
        "                                        latent_dim=latent_dimensionality,\n",
        "                                        cnn_w_regularizer=l2(lambda_cnn),\n",
        "                                        fc_w_regularizer=l2(lambda_fc),\n",
        "                                        u_w_regularizer=l2(lambda_u),\n",
        "                                        w_init = tf.keras.initializers.GlorotNormal())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "User (InputLayer)               [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "observed_image (InputLayer)     [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "non_observed_image (InputLayer) [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "User-Embedding (Embedding)      (None, 1, 50)        2259250     User[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 50)           15969138    observed_image[0][0]             \n",
            "                                                                 non_observed_image[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "FlattenUsers (Flatten)          (None, 50)           0           User-Embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract (Subtract)             (None, 50)           0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "score_rank (Dot)                (None, 1)            0           FlattenUsers[0][0]               \n",
            "                                                                 subtract[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 18,228,388\n",
            "Trainable params: 3,513,700\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60gVzsytRgj"
      },
      "source": [
        "# compile the model\n",
        "optimizer = Adam(learning_rate)\n",
        "conv_siamese_net.compile(loss=bpr_loss,\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=[auc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inX0D6EmzVaT"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9sH6jxhzjp5"
      },
      "source": [
        "# get train subset using the uniform sampling\n",
        "def get_train(user_subset, user_train):\n",
        "  train = uniform_train_sample_batch(\n",
        "      sample=False,\n",
        "      user_idx=user_subset,\n",
        "      user_train_ratings=user_train,\n",
        "      item_images=items,\n",
        "      image_width=image_width,\n",
        "      image_height=image_height)\n",
        "  return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHU99r0kifL1"
      },
      "source": [
        "# get validation subset using the uniform sampling\n",
        "def get_validation(user_subset, user_train):\n",
        "  validation = uniform_validation_sample_batch(\n",
        "      sample=False,\n",
        "      user_idx=user_subset,\n",
        "      user_train_ratings=user_train,\n",
        "      user_validation_ratings=user_validation,\n",
        "      item_images=items,\n",
        "      image_width=image_width,\n",
        "      image_height=image_height,\n",
        "      validation_sample_count=validation_sample_count)\n",
        "  return validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AMm61Ok0GhS"
      },
      "source": [
        "# create triplets lists with the user ID and the two images - train\n",
        "def get_train_triplet_lists(train):\n",
        "    user_placeholder = []\n",
        "    observed_image = []\n",
        "    not_observed_image = []\n",
        "    \n",
        "    for u, triplet in train.items():\n",
        "        user_placeholder.append(u)\n",
        "        observed_image.append(triplet[0])\n",
        "        not_observed_image.append(triplet[1])\n",
        "        \n",
        "    # label set does not exist in BPR, so we give Keras with a dummy label set\n",
        "    labels = np.ones((len(train), 1), dtype=int)\n",
        "    \n",
        "    return user_placeholder, observed_image, not_observed_image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAQs3fnG1axK"
      },
      "source": [
        "# create triplets lists with the user ID and the two images - validation\n",
        "def get_val_triplet_lists(validation):\n",
        "    user_placeholder = []\n",
        "    observed_image = []\n",
        "    not_observed_image = []\n",
        "\n",
        "    for u, triplet in validation.items():      \n",
        "      user_placeholder.append(u)\n",
        "      observed_image.append(triplet[0])\n",
        "      not_observed_image.append(triplet[1])\n",
        "    \n",
        "    # label set does not exist in BPR, so we give Keras with a dummy label set\n",
        "    labels = np.ones((len(validation), 1), dtype=int)\n",
        "\n",
        "    return user_placeholder, observed_image, not_observed_image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfp7NKehYaL5"
      },
      "source": [
        "# generate train batches throughout the training phase - yields triplet batches\n",
        "def generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    user_train = {k: v for k, v in user_train_original.items() if k in user_subset}\n",
        "    train = get_train(user_subset, user_train)\n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_train_triplet_lists(train)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "    \n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTV2psobCw9M"
      },
      "source": [
        "# generate validation batches throughout the evaluation phase - yields triplet batches\n",
        "def val_generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    validation = get_validation(user_subset, user_validation)\n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_val_triplet_lists(validation)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "\n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Z8DZf4VGuU"
      },
      "source": [
        "# data generators for training and avaluation\n",
        "train_generator = generator()\n",
        "validation_generator = val_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO-ihPTUbzuW"
      },
      "source": [
        "# conv_siamese_net.fit(x=train_generator, steps_per_epoch = num_batches, validation_data=validation_generator, validation_steps=batch_count, epochs = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYDtOBGJY77u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee803e2-d1c9-41b5-b6a6-29207c0139a8"
      },
      "source": [
        "# Training \n",
        "val_scores = []\n",
        "history = []\n",
        "model_path = \"conv_siamese_net_trained_model_implementation_20_epochs\"\n",
        "\n",
        "for i in range(2):\n",
        "  history.append(conv_siamese_net.fit(x=train_generator, steps_per_epoch = batch_count, epochs = 10)) # train\n",
        "  val_scores.append(conv_siamese_net.evaluate(validation_generator, steps = batch_count)) # validation\n",
        "  print(val_scores)\n",
        "  conv_siamese_net.save(model_path, save_traces=True, include_optimizer=True) # save model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "256/256 [==============================] - 640s 2s/step - loss: 133.6016 - auc: 0.5746\n",
            "Epoch 2/10\n",
            "256/256 [==============================] - 579s 2s/step - loss: 122.7400 - auc: 0.5932\n",
            "Epoch 3/10\n",
            "256/256 [==============================] - 579s 2s/step - loss: 121.3634 - auc: 0.6079\n",
            "Epoch 4/10\n",
            "256/256 [==============================] - 582s 2s/step - loss: 120.4659 - auc: 0.6354\n",
            "Epoch 5/10\n",
            "256/256 [==============================] - 588s 2s/step - loss: 117.0395 - auc: 0.6707\n",
            "Epoch 6/10\n",
            "256/256 [==============================] - 587s 2s/step - loss: 113.1342 - auc: 0.7054\n",
            "Epoch 7/10\n",
            "256/256 [==============================] - 586s 2s/step - loss: 108.2297 - auc: 0.7345\n",
            "Epoch 8/10\n",
            "256/256 [==============================] - 585s 2s/step - loss: 105.6024 - auc: 0.7528\n",
            "Epoch 9/10\n",
            "256/256 [==============================] - 585s 2s/step - loss: 103.3674 - auc: 0.7663\n",
            "Epoch 10/10\n",
            "256/256 [==============================] - 591s 2s/step - loss: 100.6379 - auc: 0.7829\n",
            "256/256 [==============================] - 554s 2s/step - loss: 124.8544 - auc: 0.6931\n",
            "[[124.85444641113281, 0.6930930614471436]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) User with unsupported characters which will be renamed to user in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "256/256 [==============================] - 589s 2s/step - loss: 97.8179 - auc: 0.7953\n",
            "Epoch 2/10\n",
            "256/256 [==============================] - 590s 2s/step - loss: 95.5186 - auc: 0.8052\n",
            "Epoch 3/10\n",
            "256/256 [==============================] - 589s 2s/step - loss: 95.1911 - auc: 0.8074\n",
            "Epoch 4/10\n",
            "256/256 [==============================] - 589s 2s/step - loss: 91.9723 - auc: 0.8236\n",
            "Epoch 5/10\n",
            "256/256 [==============================] - 591s 2s/step - loss: 91.9034 - auc: 0.8222\n",
            "Epoch 6/10\n",
            "256/256 [==============================] - 589s 2s/step - loss: 90.4808 - auc: 0.8316\n",
            "Epoch 7/10\n",
            "256/256 [==============================] - 591s 2s/step - loss: 89.1594 - auc: 0.8361\n",
            "Epoch 8/10\n",
            " 48/256 [====>.........................] - ETA: 7:57 - loss: 88.2493 - auc: 0.8421"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG8T-uhEfivi"
      },
      "source": [
        "# remove garbage\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g94nC0irUo22"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umOT0mBDUqfG"
      },
      "source": [
        "# load pretrained model\n",
        "model_path = \"conv_siamese_net_trained_model_ImageNet_20_epochs\"\n",
        "conv_siamese_net = tf.keras.models.load_model(model_path, custom_objects={'auc': auc, 'bpr_loss': bpr_loss})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CsD4jDpWnMF"
      },
      "source": [
        "# compile the model\n",
        "optimizer = Adam(learning_rate)\n",
        "conv_siamese_net.compile(loss=bpr_loss,\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=[auc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgeHRhrEGaN4"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r9rYRcpzndJ"
      },
      "source": [
        "# test set generator - yields batches through evaluation\n",
        "def test_generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    test = get_validation(user_subset, user_test)  \n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_val_triplet_lists(test)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "\n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBUUTmfjz5Cx"
      },
      "source": [
        "# evaluate on test set\n",
        "test_gen = test_generator()\n",
        "conv_siamese_net.evaluate(test_gen, steps = batch_count)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}