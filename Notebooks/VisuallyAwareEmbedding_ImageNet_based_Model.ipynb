{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisuallyAwareEmbedding_Generator_New (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uXGNlgsARvaV",
        "AdVO8Ok577Kp",
        "O1ISVktcRaqV",
        "BawsvBHKSL8l",
        "f1R-QW8JSYxd",
        "0u6T60gA3A1Z",
        "MyOrfAUZzYx6",
        "YSZneHJBtEDE",
        "2qmAPrZ1tO0T",
        "g94nC0irUo22",
        "JOYPOlZ_UxYe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXGNlgsARvaV"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyW-QMsbRxcU"
      },
      "source": [
        "# global libraries\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from IPython.display import display\n",
        "# Libraries\n",
        "from io import StringIO, BytesIO\n",
        "import requests\n",
        "from pytz import timezone\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm21EbS-hasl"
      },
      "source": [
        "# CNN Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmKgkm9mGYcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3bbb13c2-43eb-4d2a-e9ce-1f8bc6a8930b"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3pVLl84Ebtc"
      },
      "source": [
        "# Tensorflow libraries\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D, MaxPooling2D, BatchNormalization, Conv2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Siamese Libraries\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Lambda, Concatenate, Dot, Embedding"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVO8Ok577Kp"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGkUHV4Kas9l",
        "outputId": "f396322b-3e7b-4bfa-8afc-675de9286fd2"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnpRHkw6bA16",
        "outputId": "881ca3ea-12f1-4b47-c239-91326adba025"
      },
      "source": [
        "# change directory to dataset path\n",
        "%cd gdrive/My Drive/visually aware/visually-aware-recommender-system/notebooks"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/visually aware/visually-aware-recommender-system/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1ISVktcRaqV"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXI1fuhIRbts"
      },
      "source": [
        "# Load data\n",
        "dataset = np.load(\"AmazonMenWithImgPartitioned.npy\", \n",
        "                  encoding=\"bytes\", allow_pickle=True)\n",
        "[user_train, user_validation, user_test, items, user_num, item_num] = dataset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawsvBHKSL8l"
      },
      "source": [
        "# Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uux6JXOKSOB4"
      },
      "source": [
        "# Network params\n",
        "# image size\n",
        "image_width = 224\n",
        "image_height = 224\n",
        "\n",
        "# latent dimensionality K\n",
        "latent_dimensionality = 50\n",
        "\n",
        "# weight decay - conv layer\n",
        "lambda_cnn = 0.001\n",
        "# weight decay - fc layer\n",
        "lambda_fc = 0.001\n",
        "# regularizer for theta_u\n",
        "lambda_u = 1.0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHULKb6wAGt"
      },
      "source": [
        "# Training params\n",
        "# epoch params\n",
        "learning_rate = 1e-4\n",
        "validation_sample_count = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1R-QW8JSYxd"
      },
      "source": [
        "# ImageNet Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOJSs70YyO2U"
      },
      "source": [
        "# Transfer learning - use pretraint InceptionV3 model\n",
        "def build_inceptionV3(width, height, depth, latent_dim, w_init=\"RandomNormal\", fc_w_regularizer=None, b_init=\"RandomNormal\"):\n",
        "  input_shape = (height, width, depth)\n",
        "  model = Sequential()\n",
        "  inception = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "  inception.trainable = False\n",
        "  model.add(inception)\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(latent_dim, kernel_initializer=w_init, kernel_regularizer=fc_w_regularizer, bias_initializer=b_init))\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6T60gA3A1Z"
      },
      "source": [
        "# Subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2IdUgf_3Dli"
      },
      "source": [
        "# get user information\n",
        "user_num_original = user_num\n",
        "user_train_original = user_train"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTgT7kZf3E9J"
      },
      "source": [
        "# for each batch, force the number of users to be the same\n",
        "batch_size = 128\n",
        "batch_count = int(np.ceil(user_num_original / batch_size))\n",
        "\n",
        "# one complete model will be linked to each user_subset\n",
        "user_subsets = dict(zip(range(batch_count), np.array_split(range(user_num_original), batch_count)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOrfAUZzYx6"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey4GeNg3gq6A"
      },
      "source": [
        "# translate image from byte code to an array and normalize its values from 0-255 to -1 - 1\n",
        "def image_translate(image_bytes,\n",
        "                    image_width=224,\n",
        "                    image_height=224):\n",
        "    img = (np.uint8(np.asarray(Image.open(BytesIO(image_bytes)).convert(\"RGB\").resize((image_height, image_width)))) - 127.5)  / 127.5\n",
        "    return img"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iV5XykGzboh"
      },
      "source": [
        "# get train batches which was sampled uniformly for each user\n",
        "def uniform_train_sample_batch(user_train_ratings, item_images, image_width=224, \n",
        "                               image_height=224, sample=True, batch_size=None, user_idx=None):\n",
        "\n",
        "    if batch_size is not None:\n",
        "        users = range(batch_size)\n",
        "    else:\n",
        "        users = user_idx\n",
        "\n",
        "    triplet_train_batch = {}\n",
        "\n",
        "    # iterate over each user in subset\n",
        "    for b in users:\n",
        "\n",
        "        # training set\n",
        "        if sample:\n",
        "            u = random.randrange(len(user_train_ratings))\n",
        "        else:\n",
        "            u = b\n",
        "\n",
        "        i = user_train_ratings[u][random.randrange(len(user_train_ratings[u]))][b'productid'] # get random obesrved item\n",
        "        j = random.randrange(len(item_images)) # sample random item\n",
        "\n",
        "        # while the sampled items observed by the user - sample other item until found non-observed item\n",
        "        while j in [item[b'productid'] for item in user_train_ratings[u]]:\n",
        "            j = random.randrange(len(item_images))\n",
        "\n",
        "        image_i = image_translate(item_images[i][b'imgs'], \n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "        image_j = image_translate(item_images[j][b'imgs'],\n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "        triplet_train_batch[u] = [image_i,\n",
        "                                  image_j]\n",
        "        \n",
        "    return triplet_train_batch"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iNUZdz7YI1Q"
      },
      "source": [
        "# get validation batches which was sampled uniformly for each user\n",
        "def uniform_validation_sample_batch(user_train_ratings,\n",
        "                                    user_validation_ratings,\n",
        "                                    item_images,\n",
        "                                    image_width=224,\n",
        "                                    image_height=224,\n",
        "                                    validation_sample_count=1000,\n",
        "                                    sample=True,\n",
        "                                    batch_size=None,\n",
        "                                    user_idx=None):\n",
        "    \"\"\"\n",
        "    validation_sample_count (int): Number of not-observed items to sample to get the validation set for each user.\n",
        "    \"\"\"\n",
        "\n",
        "    if batch_size is not None:\n",
        "        users = range(batch_size)\n",
        "    else:\n",
        "        users = user_idx\n",
        "\n",
        "    triplet_validation_batch = {}\n",
        "    \n",
        "    # iterate over each user in subset\n",
        "    for b in users:\n",
        "\n",
        "        if sample:\n",
        "            u = random.randrange(len(user_train_ratings))\n",
        "        else:\n",
        "            u = b\n",
        "\n",
        "        # validation set\n",
        "        i = user_validation_ratings[u][0][b'productid'] # get the only observed item\n",
        "         # get its image\n",
        "        image_i = image_translate(item_images[i][b'imgs'], \n",
        "                                  image_width, \n",
        "                                  image_height)\n",
        "\n",
        "        # create a set of all user observed items\n",
        "        reviewed_items = set()\n",
        "        for item in user_train_ratings[u]:\n",
        "            reviewed_items.add(item[b'productid'])\n",
        "        reviewed_items.add(user_validation_ratings[u][0][b'productid'])\n",
        "\n",
        "        triplet_validation_batch[u] = []\n",
        "        \n",
        "        # search for a non-observed item\n",
        "        got = False\n",
        "        for j in random.sample(range(len(item_images)), validation_sample_count):\n",
        "            if j not in reviewed_items:\n",
        "                image_j = image_translate(item_images[j][b'imgs'],\n",
        "                                          image_width, \n",
        "                                          image_height)\n",
        "                triplet_validation_batch[u] = [image_i, image_j]\n",
        "                got = True\n",
        "            \n",
        "            if got:\n",
        "              break\n",
        "\n",
        "    return triplet_validation_batch"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1a0Sd0VxutI"
      },
      "source": [
        "# Define the loss function as ln(sigmoid) according to the BPR method\n",
        "# BPR wants to maximize the loss function while Keras engine minimizes it so we added a subtraction \n",
        "def bpr_loss(label_matrix, prediction_matrix):\n",
        "    return 1 - tf.reduce_sum(tf.math.log(tf.sigmoid(prediction_matrix)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_3LyZB0zd5q"
      },
      "source": [
        "# Count the ratio of prediction value > 0\n",
        "# i.e., predicting positive item score > negative item score for a user\n",
        "def auc(label_tensor, prediction_tensor):\n",
        "    return K.mean(K.switch(prediction_tensor > K.zeros_like(prediction_tensor),\n",
        "                           K.ones_like(prediction_tensor),    # 1\n",
        "                           K.zeros_like(prediction_tensor)))  # 0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSZneHJBtEDE"
      },
      "source": [
        "# Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixAAscXPtFKN"
      },
      "source": [
        "# build the Siamese network model\n",
        "def ConvSiameseNet(users_dim, width, height, depth,latent_dim, w_init=\"RandomNormal\", cnn_w_regularizer=None, fc_w_regularizer=None, u_w_regularizer=None, b_init=\"RandomNormal\"):\n",
        "\n",
        "        # user ID input\n",
        "        user_input = Input((1,), name='User')\n",
        "\n",
        "        image_shape = (width, height, depth)\n",
        "\n",
        "        # observed image\n",
        "        left_input = Input(image_shape,\n",
        "                           name=\"observed_image\")\n",
        "        # non-observed image\n",
        "        right_input = Input(image_shape,\n",
        "                            name=\"non_observed_image\")\n",
        "\n",
        "        # Build convnet to use in each siamese 'leg'\n",
        "        conv_net = build_inceptionV3(width, height, depth, latent_dim, w_init, fc_w_regularizer, b_init)\n",
        "\n",
        "        # preference layer\n",
        "        preference_relationship = Dot(axes=1, name=\"score_rank\")\n",
        "\n",
        "        # Apply the pipeline to the inputs\n",
        "        # call the convnet Sequential model on each of the input tensors\n",
        "        # so params will be shared\n",
        "        encoded_l = conv_net(left_input)\n",
        "        encoded_r = conv_net(right_input)\n",
        "\n",
        "        # merge the two encoded inputs through the L1 distance\n",
        "        L1_dist = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
        "        \n",
        "        # retrieve the single user preference\n",
        "        user_vec = Flatten(name='FlattenUsers')(Embedding(user_num+1, latent_dimensionality, name = 'User-Embedding', embeddings_initializer= tf.keras.initializers.RandomUniform(0,0.01),\n",
        "                                                          input_length=1, embeddings_regularizer=u_w_regularizer)(user_input))\n",
        "        \n",
        "        # get the preference score\n",
        "        prediction = preference_relationship([user_vec, L1_dist])\n",
        "        \n",
        "        # Create the model\n",
        "        model = Model(inputs=[user_input, left_input, right_input],\n",
        "                      outputs=prediction)\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        \n",
        "        return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qmAPrZ1tO0T"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDlrHUUitOdD",
        "outputId": "10f03891-9104-4ee3-bdbf-320e80df08f4"
      },
      "source": [
        "# build the model with the regularization hyper-parameters and initializers\n",
        "conv_siamese_net = ConvSiameseNet(users_dim=len(user_subsets[0]),\n",
        "                                        width=image_width,\n",
        "                                        height=image_height,\n",
        "                                        depth=3,\n",
        "                                        latent_dim=latent_dimensionality,\n",
        "                                        cnn_w_regularizer=l2(lambda_cnn),\n",
        "                                        fc_w_regularizer=l2(lambda_fc),\n",
        "                                        u_w_regularizer=l2(lambda_u),\n",
        "                                        w_init = tf.keras.initializers.GlorotNormal())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "User (InputLayer)               [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "observed_image (InputLayer)     [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "non_observed_image (InputLayer) [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "User-Embedding (Embedding)      (None, 1, 50)        1712250     User[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 50)           24362834    observed_image[0][0]             \n",
            "                                                                 non_observed_image[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "FlattenUsers (Flatten)          (None, 50)           0           User-Embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract (Subtract)             (None, 50)           0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "score_rank (Dot)                (None, 1)            0           FlattenUsers[0][0]               \n",
            "                                                                 subtract[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 26,075,084\n",
            "Trainable params: 4,272,300\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60gVzsytRgj"
      },
      "source": [
        "# compile the model\n",
        "optimizer = Adam(learning_rate)\n",
        "conv_siamese_net.compile(loss=bpr_loss,\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=[auc])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inX0D6EmzVaT"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9sH6jxhzjp5"
      },
      "source": [
        "# get train subset using the uniform sampling\n",
        "def get_train(user_subset, user_train):\n",
        "  train = uniform_train_sample_batch(\n",
        "      sample=False,\n",
        "      user_idx=user_subset,\n",
        "      user_train_ratings=user_train,\n",
        "      item_images=items,\n",
        "      image_width=image_width,\n",
        "      image_height=image_height)\n",
        "  return train"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHU99r0kifL1"
      },
      "source": [
        "# get validation subset using the uniform sampling\n",
        "def get_validation(user_subset, user_train):\n",
        "  validation = uniform_validation_sample_batch(\n",
        "      sample=False,\n",
        "      user_idx=user_subset,\n",
        "      user_train_ratings=user_train,\n",
        "      user_validation_ratings=user_validation,\n",
        "      item_images=items,\n",
        "      image_width=image_width,\n",
        "      image_height=image_height,\n",
        "      validation_sample_count=validation_sample_count)\n",
        "  return validation"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AMm61Ok0GhS"
      },
      "source": [
        "# create triplets lists with the user ID and the two images - train\n",
        "def get_train_triplet_lists(train):\n",
        "    user_placeholder = []\n",
        "    observed_image = []\n",
        "    not_observed_image = []\n",
        "    \n",
        "    for u, triplet in train.items():\n",
        "        user_placeholder.append(u)\n",
        "        observed_image.append(triplet[0])\n",
        "        not_observed_image.append(triplet[1])\n",
        "        \n",
        "    # label set does not exist in BPR, so we give Keras with a dummy label set\n",
        "    labels = np.ones((len(train), 1), dtype=int)\n",
        "    \n",
        "    return user_placeholder, observed_image, not_observed_image, labels"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAQs3fnG1axK"
      },
      "source": [
        "# create triplets lists with the user ID and the two images - validation\n",
        "def get_val_triplet_lists(validation):\n",
        "    user_placeholder = []\n",
        "    observed_image = []\n",
        "    not_observed_image = []\n",
        "\n",
        "    for u, triplet in validation.items():      \n",
        "      user_placeholder.append(u)\n",
        "      observed_image.append(triplet[0])\n",
        "      not_observed_image.append(triplet[1])\n",
        "      \n",
        "    # label set does not exist in BPR, so we give Keras with a dummy label set\n",
        "    labels = np.ones((len(validation), 1), dtype=int)\n",
        "\n",
        "    return user_placeholder, observed_image, not_observed_image, labels"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfp7NKehYaL5"
      },
      "source": [
        "# generate train batches throughout the training phase - yields triplet batches\n",
        "def generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    user_train = {k: v for k, v in user_train_original.items() if k in user_subset}\n",
        "    train = get_train(user_subset, user_train)\n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_train_triplet_lists(train)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "    \n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTV2psobCw9M"
      },
      "source": [
        "# generate validation batches throughout the evaluation phase - yields triplet batches\n",
        "def val_generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    validation = get_validation(user_subset, user_validation)\n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_val_triplet_lists(validation)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "\n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Z8DZf4VGuU"
      },
      "source": [
        "# data generators for training and avaluation\n",
        "train_generator = generator()\n",
        "validation_generator = val_generator()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO-ihPTUbzuW"
      },
      "source": [
        "# conv_siamese_net.fit(x=train_generator, steps_per_epoch = num_batches, validation_data=validation_generator, validation_steps=batch_count, epochs = 20)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYDtOBGJY77u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0b36ad-0fab-4230-d960-bb797307ca20"
      },
      "source": [
        "# Training \n",
        "val_scores = []\n",
        "history = []\n",
        "model_path = \"conv_siamese_net_trained_model_ImageNet_20_epochs\"\n",
        "\n",
        "for i in range(2):\n",
        "  history.append(conv_siamese_net.fit(x=train_generator, steps_per_epoch = batch_count, epochs = 10)) # train\n",
        "  val_scores.append(conv_siamese_net.evaluate(validation_generator, steps = batch_count)) # validation\n",
        "  print(f\"validation score: {val_scores}\") \n",
        "  conv_siamese_net.save(model_path, save_traces=True, include_optimizer=True) # save model"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "268/268 [==============================] - 422s 1s/step - loss: 98.1863 - auc: 0.5735\n",
            "Epoch 2/10\n",
            "268/268 [==============================] - 375s 1s/step - loss: 88.6041 - auc: 0.6182\n",
            "Epoch 3/10\n",
            "268/268 [==============================] - 373s 1s/step - loss: 83.5907 - auc: 0.6771\n",
            "Epoch 4/10\n",
            "268/268 [==============================] - 374s 1s/step - loss: 78.8341 - auc: 0.7270\n",
            "Epoch 5/10\n",
            "268/268 [==============================] - 377s 1s/step - loss: 73.7081 - auc: 0.7702\n",
            "Epoch 6/10\n",
            "268/268 [==============================] - 374s 1s/step - loss: 69.8830 - auc: 0.7995\n",
            "Epoch 7/10\n",
            "268/268 [==============================] - 377s 1s/step - loss: 66.6871 - auc: 0.8210\n",
            "Epoch 8/10\n",
            "268/268 [==============================] - 381s 1s/step - loss: 63.4660 - auc: 0.8399\n",
            "Epoch 9/10\n",
            "268/268 [==============================] - 377s 1s/step - loss: 61.0695 - auc: 0.8516\n",
            "Epoch 10/10\n",
            "268/268 [==============================] - 373s 1s/step - loss: 59.1119 - auc: 0.8622\n",
            "268/268 [==============================] - 356s 1s/step - loss: 97.8938 - auc: 0.6985\n",
            "validation score: [[97.89378356933594, 0.6984928250312805]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) User with unsupported characters which will be renamed to user in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "268/268 [==============================] - 382s 1s/step - loss: 57.7197 - auc: 0.8697\n",
            "Epoch 2/10\n",
            "268/268 [==============================] - 382s 1s/step - loss: 55.6358 - auc: 0.8815\n",
            "Epoch 3/10\n",
            "268/268 [==============================] - 388s 1s/step - loss: 54.4811 - auc: 0.8865\n",
            "Epoch 4/10\n",
            "268/268 [==============================] - 381s 1s/step - loss: 53.3653 - auc: 0.8932\n",
            "Epoch 5/10\n",
            "268/268 [==============================] - 379s 1s/step - loss: 52.3402 - auc: 0.8958\n",
            "Epoch 6/10\n",
            "268/268 [==============================] - 377s 1s/step - loss: 51.7496 - auc: 0.8997\n",
            "Epoch 7/10\n",
            "268/268 [==============================] - 377s 1s/step - loss: 50.6808 - auc: 0.9030\n",
            "Epoch 8/10\n",
            "268/268 [==============================] - 378s 1s/step - loss: 49.4612 - auc: 0.9081\n",
            "Epoch 9/10\n",
            "268/268 [==============================] - 378s 1s/step - loss: 48.8940 - auc: 0.9093\n",
            "Epoch 10/10\n",
            "268/268 [==============================] - 380s 1s/step - loss: 48.8370 - auc: 0.9102\n",
            "268/268 [==============================] - 348s 1s/step - loss: 102.4114 - auc: 0.7091\n",
            "validation score: [[97.89378356933594, 0.6984928250312805], [102.41142272949219, 0.7091496586799622]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) User with unsupported characters which will be renamed to user in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: conv_siamese_net_trained_model_ImageNet_30_epochs/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUjmXHcFVBIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c81b202-1d92-4e56-8370-0abaaee5eacb"
      },
      "source": [
        "# remove garbage\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "724471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g94nC0irUo22"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umOT0mBDUqfG"
      },
      "source": [
        "# load pretrained model\n",
        "model_path = \"conv_siamese_net_trained_model_ImageNet_20_epochs\"\n",
        "conv_siamese_net = tf.keras.models.load_model(model_path, custom_objects={'auc': auc, 'bpr_loss': bpr_loss})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CsD4jDpWnMF"
      },
      "source": [
        "# compile the model\n",
        "optimizer = Adam(learning_rate)\n",
        "conv_siamese_net.compile(loss=bpr_loss,\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=[auc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKjq9Oa_EK3_"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnIUoEAjEKVf"
      },
      "source": [
        "# test set generator - yields batches through evaluation\n",
        "def test_generator():\n",
        "  batches = range(batch_count)\n",
        "  num_batches_in_curr_epoch = 0\n",
        "  user_subset_idx = 0\n",
        "  while True:\n",
        "    user_subset = user_subsets[user_subset_idx] # get batch users\n",
        "    test = get_validation(user_subset, user_test)  \n",
        "    user_placeholder, observed_image, not_observed_image, labels = get_val_triplet_lists(test)\n",
        "    \n",
        "    yield [np.asarray(user_placeholder), np.asarray(observed_image), np.asarray(not_observed_image)], np.asarray(labels)\n",
        "\n",
        "    num_batches_in_curr_epoch+=1\n",
        "    user_subset_idx+=1\n",
        "    \n",
        "    if num_batches_in_curr_epoch >= batch_count:\n",
        "      # reset the lists\n",
        "      num_batches_in_curr_epoch = 0\n",
        "      user_subset_idx = 0"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgj53TAGIgFG",
        "outputId": "d17225ab-7fad-4b06-d7b5-9caf6e210576"
      },
      "source": [
        "# evaluate on test set\n",
        "test_gen = test_generator()\n",
        "conv_siamese_net.evaluate(test_gen, steps = batch_count)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "268/268 [==============================] - 355s 1s/step - loss: 103.1939 - auc: 0.7071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[103.19387817382812, 0.7071347832679749]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}